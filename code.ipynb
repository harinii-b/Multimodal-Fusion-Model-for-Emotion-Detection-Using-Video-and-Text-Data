{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "name": "PESU_RR_2_190_193_223_231"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 88291,
          "databundleVersionId": 10117909,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "UCIwGymzPZBM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "pes_ml_hack_link1_path = kagglehub.competition_download('PES-ml-hack-link1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "IN2BUA2aPZBW"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "9bOgCZEZOZhM",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:46:06.833368Z",
          "iopub.execute_input": "2024-11-11T15:46:06.834106Z",
          "iopub.status.idle": "2024-11-11T15:46:07.264227Z",
          "shell.execute_reply.started": "2024-11-11T15:46:06.834053Z",
          "shell.execute_reply": "2024-11-11T15:46:07.263058Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/train.csv',encoding='cp1252')\n",
        "\n",
        "# Define path to video clips\n",
        "\n",
        "video_dir = '/kaggle/input/PES-ml-hack-link1/train_videos'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to get video file path from IDs\n",
        "\n",
        "def get_video_clip_path(row):\n",
        "\n",
        "    dialogue_id = row['Dialogue_ID']\n",
        "\n",
        "    utterance_id = row['Utterance_ID']\n",
        "\n",
        "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
        "\n",
        "    return os.path.join(video_dir, filename)\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function to get file paths for each sampled clip\n",
        "\n",
        "train_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Check sample paths\n",
        "\n",
        "print(train_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())"
      ],
      "metadata": {
        "id": "oJDiug6BOZhT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:46:09.037165Z",
          "iopub.execute_input": "2024-11-11T15:46:09.038075Z",
          "iopub.status.idle": "2024-11-11T15:46:09.076232Z",
          "shell.execute_reply.started": "2024-11-11T15:46:09.038031Z",
          "shell.execute_reply": "2024-11-11T15:46:09.075127Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "ZdBbGM-ROZhV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:46:10.806394Z",
          "iopub.execute_input": "2024-11-11T15:46:10.807477Z",
          "iopub.status.idle": "2024-11-11T15:46:10.815262Z",
          "shell.execute_reply.started": "2024-11-11T15:46:10.807429Z",
          "shell.execute_reply": "2024-11-11T15:46:10.814051Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESSING STAGE**"
      ],
      "metadata": {
        "id": "bSLU92ZeAONj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of records per emotion class\n",
        "\n",
        "\n",
        "\n",
        "if train_df.isnull().sum().any():\n",
        "\n",
        "    print(\"Dataset contains missing values!\")\n",
        "\n",
        "else:\n",
        "\n",
        "    print(\"No missing values found.\")\n",
        "\n",
        "\n",
        "\n",
        "class_counts = train_df['Emotion'].value_counts()\n",
        "\n",
        "print(\"Class distribution:\")\n",
        "\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "YCbH3g0bBEIZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:46:13.893724Z",
          "iopub.execute_input": "2024-11-11T15:46:13.894134Z",
          "iopub.status.idle": "2024-11-11T15:46:13.903729Z",
          "shell.execute_reply.started": "2024-11-11T15:46:13.894094Z",
          "shell.execute_reply": "2024-11-11T15:46:13.902683Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checked for null values: Ensured that there were **no missing values** in the dataset.\n",
        "\n",
        "\n",
        "\n",
        "Since there are no null values, proceeded to check if the dataset is **biased**.\n",
        "\n",
        "\n",
        "\n",
        "The distribution of records across different emotion classes in the dataset is calculated and we can infer that there is **class imbalance** because out of the 5 classes, around **half** the dataset values belong to **neutral class**.\n"
      ],
      "metadata": {
        "id": "GrAF1vWtVAKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:48:51.948631Z",
          "iopub.execute_input": "2024-11-11T15:48:51.949097Z",
          "iopub.status.idle": "2024-11-11T15:49:26.714215Z",
          "shell.execute_reply.started": "2024-11-11T15:48:51.949049Z",
          "shell.execute_reply": "2024-11-11T15:49:26.712723Z"
        },
        "id": "5vEvkHHdPZBq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  EMOTION ANALYSIS FROM TEXT"
      ],
      "metadata": {
        "id": "ELKc67v56_-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Count the number of records per emotion class\n",
        "\n",
        "class_counts = train_df['Emotion'].value_counts()\n",
        "\n",
        "print(\"Original class distribution:\")\n",
        "\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "xOpID-SKOndD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:49:43.402008Z",
          "iopub.execute_input": "2024-11-11T15:49:43.402512Z",
          "iopub.status.idle": "2024-11-11T15:49:43.414864Z",
          "shell.execute_reply.started": "2024-11-11T15:49:43.402468Z",
          "shell.execute_reply": "2024-11-11T15:49:43.413575Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabet characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize, remove stopwords, and lemmatize using spaCy\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in STOP_WORDS and not token.is_punct]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the function\n",
        "train_df['Processed_Utterance'] = train_df['Utterance'].apply(preprocess_text)\n",
        "\n",
        "# Separate features and target\n",
        "X = train_df['Processed_Utterance']\n",
        "y = train_df['Emotion']\n"
      ],
      "metadata": {
        "id": "0n6gGJY8PSjo",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:49:48.142324Z",
          "iopub.execute_input": "2024-11-11T15:49:48.14276Z",
          "iopub.status.idle": "2024-11-11T15:49:57.852761Z",
          "shell.execute_reply.started": "2024-11-11T15:49:48.14272Z",
          "shell.execute_reply": "2024-11-11T15:49:57.85162Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `preprocess_text` function converts text to lowercase, removes non-alphabetic characters, eliminates stopwords, and applies lemmatization (converts to base or root form), returning cleaned text for further analysis.This new column, Processed_Uttterance will be used as the model’s input feature instead of the raw text.\n",
        "\n",
        "\n",
        "\n",
        "X contains preprocessed utterances, and y contains the target emotion class for each instance."
      ],
      "metadata": {
        "id": "8GKXW9ZvPTpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the processed text\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X_tokenized = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=100)\n",
        "\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n"
      ],
      "metadata": {
        "id": "OAsxojy2QkCv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:50:02.207144Z",
          "iopub.execute_input": "2024-11-11T15:50:02.207879Z",
          "iopub.status.idle": "2024-11-11T15:50:02.24341Z",
          "shell.execute_reply.started": "2024-11-11T15:50:02.207832Z",
          "shell.execute_reply": "2024-11-11T15:50:02.242339Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The code tokenizes the processed text into sequences and pads them to a\n",
        "\n",
        "fixed length of 100. This prepares X for input into a neural network by converting it to a consistent numerical format.\n",
        "\n",
        "\n",
        "\n",
        "*   Labels (y) are encoded into numerical values for model compatibility, allowing the model to learn and classify emotions based on the input features.\n",
        "\n",
        "* The data is split into training and validation sets with an 80-20 ratio. Stratified splitting ensures an even class distribution across both sets, helping the model generalize well on unseen data."
      ],
      "metadata": {
        "id": "Op7rvEVjQlXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Embedding(input_dim=10000, output_dim=128, input_length=100))\n",
        "\n",
        "    model.add(layers.LSTM(64, return_sequences=True))\n",
        "\n",
        "    model.add(layers.LSTM(32))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "    model.add(layers.Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "\n",
        "\n",
        "class_weights = {i: 1.0 / count for i, count in enumerate(class_counts)}\n",
        "\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, class_weight=class_weights)\n",
        "\n",
        "\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "\n",
        "print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')"
      ],
      "metadata": {
        "id": "yT9WbIJERSET",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:50:07.584368Z",
          "iopub.execute_input": "2024-11-11T15:50:07.584798Z",
          "iopub.status.idle": "2024-11-11T15:50:33.948845Z",
          "shell.execute_reply.started": "2024-11-11T15:50:07.584758Z",
          "shell.execute_reply": "2024-11-11T15:50:33.947754Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have built an lstm model due to the following reasons:\n",
        "\n",
        "1. **Sequential Data**: LSTMs capture word order and context, which is crucial for text, whereas SVMs treat features independently.\n",
        "\n",
        "\n",
        "\n",
        "2. **Feature Learning**: LSTMs learn relevant features automatically, while SVMs require manual feature engineering.\n",
        "\n",
        "\n",
        "\n",
        "3. **Dimensionality**: LSTMs handle high-dimensional data (large vocabularies) better, while SVMs may struggle without special tuning.\n",
        "\n",
        "\n",
        "\n",
        "4. **Probabilities**: LSTMs provide class probabilities, simplifying class balancing, while SVMs don’t naturally offer probabilities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Model specifications:\n",
        "\n",
        "- Embedding Layer: Converts each word in the sequence to a 128-dimensional vector, capturing word relationships and semantic meaning.\n",
        "\n",
        "\n",
        "\n",
        "- LSTM Layers: Two LSTM layers (64 and 32 units) capture temporal dependencies and sequential patterns in the text.\n",
        "\n",
        "\n",
        "\n",
        "- Dense Layers: A hidden dense layer (64 units, ReLU activation) helps learn non-linear relationships, and the output layer (softmax activation) predicts probabilities for each emotion class.\n",
        "\n",
        "\n",
        "\n",
        "- Compilation: The model uses the Adam optimizer sparse_categorical_crossentropy loss for multi-class classification, and accuracy as the performance metric.\n",
        "\n",
        "\n",
        "\n",
        "**Class Weights** adjusts the model to give more importance to minority classes by inversely scaling the weights based on class frequencies. This helps address **class imbalance**, improving model fairness across all emotions.\n",
        "\n",
        "\n",
        "\n",
        "The model trains on X_train and y_train for 10 epochs with a batch size of\n",
        "\n",
        "\n",
        "\n",
        "1.   List item\n",
        "\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n",
        "32, using X_val and y_val for validation. Class weights are applied to improve model balance across classes.\n",
        "\n",
        "\n",
        "\n",
        "After training, the model is evaluated on validation data, providing val_loss and val_accuracy as metrics to assess model performance and generalization."
      ],
      "metadata": {
        "id": "_wSu7T_RSuYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EMOTION ANALYSIS FROM VIDEO**"
      ],
      "metadata": {
        "id": "0HFU66-yWHiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Load and preprocess the train.csv\n",
        "\n",
        "def preprocess_data(csv_path='/kaggle/input/PES-ml-hack-link1/train_videos'):\n",
        "\n",
        "    # Load the train.csv file\n",
        "\n",
        "    train_data = pd.read_csv(csv_path,encoding='cp1252')\n",
        "\n",
        "\n",
        "\n",
        "    # Extract relevant columns: 'Utterance_ID', 'Dialogue_ID', 'Emotion'\n",
        "\n",
        "    train_data = train_data[['Utterance_ID', 'Dialogue_ID', 'Emotion']]\n",
        "\n",
        "\n",
        "\n",
        "    # Map emotions to labels\n",
        "\n",
        "    emotion_map = {'anger': 0, 'joy': 1, 'neutral': 2, 'sadness': 3, 'surprise': 4}\n",
        "\n",
        "    train_data['Emotion_Label'] = train_data['Emotion'].map(emotion_map)\n",
        "\n",
        "\n",
        "\n",
        "    return train_data\n"
      ],
      "metadata": {
        "id": "jkvveWeUWQba",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:50:38.460796Z",
          "iopub.execute_input": "2024-11-11T15:50:38.461223Z",
          "iopub.status.idle": "2024-11-11T15:50:38.683556Z",
          "shell.execute_reply.started": "2024-11-11T15:50:38.461183Z",
          "shell.execute_reply": "2024-11-11T15:50:38.682364Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Create CNN Model for feature extraction\n",
        "\n",
        "def create_cnn_model(input_shape=(64, 64, 3)):\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "\n",
        "\n",
        "    # First convolutional block\n",
        "\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "\n",
        "\n",
        "    # Second convolutional block\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "\n",
        "\n",
        "    # Third convolutional block\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "\n",
        "\n",
        "    # Flatten the feature maps\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "\n",
        "\n",
        "    # Fully connected layer for feature extraction\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "    # The model ends here for feature extraction purpose\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "9s1-ZpbTic06",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:50:42.432555Z",
          "iopub.execute_input": "2024-11-11T15:50:42.433596Z",
          "iopub.status.idle": "2024-11-11T15:50:42.44217Z",
          "shell.execute_reply.started": "2024-11-11T15:50:42.433549Z",
          "shell.execute_reply": "2024-11-11T15:50:42.440808Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CNN Model for Feature Extraction**\n",
        "\n",
        "\n",
        "\n",
        "1. **Convolutional Blocks**:\n",
        "\n",
        "   - Three convolutional blocks with increasing filters (32, 64, 128), each followed by max-pooling to extract spatial features.\n",
        "\n",
        "\n",
        "\n",
        "2. **Flattening**:\n",
        "\n",
        "   - Flatten the feature maps into a 1D vector.\n",
        "\n",
        "\n",
        "\n",
        "3. **Fully Connected Layer**:\n",
        "\n",
        "   - A dense layer (128 units) with ReLU activation and a dropout layer to prevent overfitting.\n",
        "\n",
        "\n",
        "\n",
        "4. **Output**:\n",
        "\n",
        "   - The model is designed for feature extraction and ends before the classification layer.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "C0y1Z89-lJ7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 3: Extract frames from video\n",
        "\n",
        "def extract_frames_from_video(video_path, frame_size=(64, 64)):\n",
        "\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frames = []\n",
        "\n",
        "\n",
        "\n",
        "    while video_capture.isOpened():\n",
        "\n",
        "        ret, frame = video_capture.read()\n",
        "\n",
        "        if not ret:\n",
        "\n",
        "            break\n",
        "\n",
        "        # Resize and normalize the frames\n",
        "\n",
        "        frame = cv2.resize(frame, frame_size)\n",
        "\n",
        "        frame = frame / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "\n",
        "\n",
        "    video_capture.release()\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Get video features using the CNN model\n",
        "\n",
        "def get_video_features(dialogue_id, utterance_id, cnn_model, video_dir='videos'):\n",
        "\n",
        "    video_filename = f'dia{dialogue_id}_utt{utterance_id}.mp4'\n",
        "\n",
        "    video_path = os.path.join(video_dir, video_filename)\n",
        "\n",
        "\n",
        "\n",
        "    if os.path.exists(video_path):\n",
        "\n",
        "        frames = extract_frames_from_video(video_path)\n",
        "\n",
        "        # Extract features using the CNN model\n",
        "\n",
        "        features = cnn_model.predict(frames)\n",
        "\n",
        "        return np.mean(features, axis=0)  # Use the average of the frame features\n",
        "\n",
        "    else:\n",
        "\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "yfKYULj4ieWj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:50:46.900503Z",
          "iopub.execute_input": "2024-11-11T15:50:46.90093Z",
          "iopub.status.idle": "2024-11-11T15:50:46.909571Z",
          "shell.execute_reply.started": "2024-11-11T15:50:46.900888Z",
          "shell.execute_reply": "2024-11-11T15:50:46.908356Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Video Frame Extraction and Feature Extraction Using CNN**\n",
        "\n",
        "\n",
        "\n",
        "1. **Extract Frames from Video**:\n",
        "\n",
        "   - This function reads video files frame by frame.\n",
        "\n",
        "   - Each frame is resized to a specified size (default is 64x64 pixels).\n",
        "\n",
        "   - The pixel values of the frames are normalized to the range `[0, 1]` for consistency across videos.\n",
        "\n",
        "\n",
        "\n",
        "2. **Get Video Features Using CNN**:\n",
        "\n",
        "   - For each video, the function constructs the video file path using `Dialogue_ID` and `Utterance_ID`.\n",
        "\n",
        "   - It extracts frames from the video using the previously defined frame extraction function.\n",
        "\n",
        "   - The frames are then passed through a pre-trained CNN model to extract features.\n",
        "\n",
        "   - The extracted features are averaged over all frames to create a single feature vector representing the video.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-TImzqODkbdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 5: Prepare data for training the emotion recognition model\n",
        "\n",
        "def prepare_data(train_data, cnn_model, video_dir='videos'):\n",
        "\n",
        "    X = []\n",
        "\n",
        "    y = []\n",
        "\n",
        "\n",
        "\n",
        "    for _, row in train_data.iterrows():\n",
        "\n",
        "        features = get_video_features(row['Dialogue_ID'], row['Utterance_ID'], cnn_model, video_dir)\n",
        "\n",
        "        if features is not None:\n",
        "\n",
        "            X.append(features)\n",
        "\n",
        "            y.append(row['Emotion_Label'])\n",
        "\n",
        "\n",
        "\n",
        "    X = np.array(X)\n",
        "\n",
        "    y = np.array(y)\n",
        "\n",
        "\n",
        "\n",
        "    # One-hot encode the labels\n",
        "\n",
        "    y = to_categorical(y, num_classes=5)\n",
        "\n",
        "\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "# Step 6: Create the Emotion Recognition Model\n",
        "\n",
        "def create_emotion_recognition_model(input_shape=(64,)):\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "\n",
        "\n",
        "    # Fully connected layers for emotion classification\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu', input_shape=input_shape))\n",
        "\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "    # Output layer for emotion classification (5 classes)\n",
        "\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4Bp-4zpXikeU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:50:51.944772Z",
          "iopub.execute_input": "2024-11-11T15:50:51.946289Z",
          "iopub.status.idle": "2024-11-11T15:50:51.955608Z",
          "shell.execute_reply.started": "2024-11-11T15:50:51.946205Z",
          "shell.execute_reply": "2024-11-11T15:50:51.954568Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Emotion Recognition Model Setup**\n",
        "\n",
        "\n",
        "\n",
        "1. **Prepare Data for Training**:\n",
        "\n",
        "   - This function processes the training data by:\n",
        "\n",
        "     - Extracting video features using the pre-trained CNN model for each video.\n",
        "\n",
        "     - Collecting the features and corresponding emotion labels.\n",
        "\n",
        "     - One-hot encoding the emotion labels to prepare them for classification.\n",
        "\n",
        "\n",
        "\n",
        "2. **Create the Emotion Recognition Model**:\n",
        "\n",
        "   - A fully connected neural network model is defined for emotion classification:\n",
        "\n",
        "     - The first layer is a Dense layer with 128 units and ReLU activation.\n",
        "\n",
        "     - A Dropout layer is added to prevent overfitting.\n",
        "\n",
        "     - A second Dense layer with 64 units and ReLU activation.\n",
        "\n",
        "     - The output layer consists of 5 units (one for each emotion) with a softmax activation function for multi-class classification.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "R2ND5fe-koyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 7: Main script to train the model\n",
        "\n",
        "def train_emotion_recognition_model(csv_path='/kaggle/input/PES-ml-hack-link1/train.csv', video_dir='videos'):\n",
        "\n",
        "    # Preprocess the data\n",
        "\n",
        "    train_data = preprocess_data(csv_path)\n",
        "\n",
        "\n",
        "\n",
        "    # Create CNN model for feature extraction\n",
        "\n",
        "    cnn_model = create_cnn_model(input_shape=(64, 64, 3))\n",
        "\n",
        "\n",
        "\n",
        "    # Prepare features and labels\n",
        "\n",
        "    X, y = prepare_data(train_data, cnn_model, video_dir)\n",
        "\n",
        "\n",
        "\n",
        "    # Split into training and validation sets\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "    # Create and compile the emotion recognition model\n",
        "\n",
        "    emotion_model = create_emotion_recognition_model(input_shape=(X.shape[1],))\n",
        "\n",
        "    emotion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "\n",
        "    emotion_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "\n",
        "    val_loss, val_accuracy = emotion_model.evaluate(X_val, y_val)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "\n",
        "\n",
        "# Run the training process\n",
        "\n",
        "train_emotion_recognition_model(csv_path='/kaggle/input/PES-ml-hack-link1/train.csv', video_dir='/kaggle/input/PES-ml-hack-link1/train_videos')"
      ],
      "metadata": {
        "id": "HDFKgSvxiopR",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:50:57.032462Z",
          "iopub.execute_input": "2024-11-11T15:50:57.032899Z",
          "iopub.status.idle": "2024-11-11T15:57:01.594717Z",
          "shell.execute_reply.started": "2024-11-11T15:50:57.032844Z",
          "shell.execute_reply": "2024-11-11T15:57:01.59371Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training the Emotion Recognition Model**\n",
        "\n",
        "\n",
        "\n",
        "1. **Preprocess Data**:\n",
        "\n",
        "   - The data is preprocessed, extracting video features and one-hot encoded emotion labels.\n",
        "\n",
        "\n",
        "\n",
        "2. **Feature Extraction**:\n",
        "\n",
        "   - A CNN model is created to extract features from video frames.\n",
        "\n",
        "\n",
        "\n",
        "3. **Prepare Features and Labels**:\n",
        "\n",
        "   - Features and labels are prepared for training, then split into training and validation sets.\n",
        "\n",
        "\n",
        "\n",
        "4. **Create and Compile Model**:\n",
        "\n",
        "   - An emotion recognition model with fully connected layers is created and compiled.\n",
        "\n",
        "\n",
        "\n",
        "5. **Train Model**:\n",
        "\n",
        "   - The model is trained for 10 epochs, using the training data and validating on the validation set.\n",
        "\n",
        "\n",
        "\n",
        "6. **Evaluate Model**:\n",
        "\n",
        "   - The model is evaluated on the validation set, with the loss and accuracy printed.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DkYOmjwpk5Bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMBINING TEXT AND VIDEO MODELS**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tn7oGmk-TQtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "j4gmeR5ckHdQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:57:46.329588Z",
          "iopub.execute_input": "2024-11-11T15:57:46.330808Z",
          "iopub.status.idle": "2024-11-11T15:57:46.340775Z",
          "shell.execute_reply.started": "2024-11-11T15:57:46.330753Z",
          "shell.execute_reply": "2024-11-11T15:57:46.339514Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Preprocessing function for text\n",
        "\n",
        "# def preprocess_text(text):\n",
        "\n",
        "#     text = text.lower()\n",
        "\n",
        "#     text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "#     tokens = text.split()\n",
        "\n",
        "#     stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "#     tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "#     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "#     return ' '.join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# # Apply preprocessing to the utterances\n",
        "\n",
        "# train_df['Processed_Utterance'] = train_df['Utterance'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "\n",
        "# # Separate features and labels\n",
        "\n",
        "# X = train_df['Processed_Utterance']\n",
        "\n",
        "# y = train_df['Emotion']\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabet characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize, remove stopwords, and lemmatize using spaCy\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in STOP_WORDS and not token.is_punct]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the function\n",
        "train_df['Processed_Utterance'] = train_df['Utterance'].apply(preprocess_text)\n",
        "\n",
        "# Separate features and target\n",
        "X = train_df['Processed_Utterance']\n",
        "y = train_df['Emotion']\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the processed text\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X_tokenized = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=100)\n",
        "\n",
        "\n",
        "\n",
        "# Encode emotion labels\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "\n",
        "\n",
        "# Define the text model\n",
        "\n",
        "def create_text_model():\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Embedding(input_dim=10000, output_dim=128, input_length=100))\n",
        "\n",
        "    model.add(layers.LSTM(64, return_sequences=True))\n",
        "\n",
        "    model.add(layers.LSTM(32))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "    model.add(layers.Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Create the text model\n",
        "\n",
        "text_model = create_text_model()\n",
        "\n",
        "\n",
        "\n",
        "# Calculate class weights\n",
        "\n",
        "class_counts = train_df['Emotion'].value_counts()\n",
        "\n",
        "class_weights = {i: 1.0 / count for i, count in enumerate(class_counts)}\n",
        "\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "\n",
        "\n",
        "# Train the text model with class weights\n",
        "\n",
        "history_text = text_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, class_weight=class_weights)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the text model on the validation set\n",
        "\n",
        "val_loss_text, val_accuracy_text = text_model.evaluate(X_val, y_val)\n",
        "\n",
        "print(f'Text Model - Validation Loss: {val_loss_text}, Validation Accuracy: {val_accuracy_text}')\n"
      ],
      "metadata": {
        "id": "vNk4rqukgBFE",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:59:00.187108Z",
          "iopub.execute_input": "2024-11-11T15:59:00.187959Z",
          "iopub.status.idle": "2024-11-11T15:59:32.466582Z",
          "shell.execute_reply.started": "2024-11-11T15:59:00.187913Z",
          "shell.execute_reply": "2024-11-11T15:59:32.465455Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Text Preprocessing & Model Training**\n",
        "\n",
        "\n",
        "\n",
        "1. **Preprocessing**:\n",
        "\n",
        "   - Text is cleaned (lowercased, special chars removed), tokenized, stop words removed, and lemmatized.\n",
        "\n",
        "\n",
        "\n",
        "2. **Data Preparation**:\n",
        "\n",
        "   - Features (`X`) are tokenized and padded; labels (`y`) are encoded using `LabelEncoder`.\n",
        "\n",
        "\n",
        "\n",
        "3. **Model**:\n",
        "\n",
        "   - A deep learning model is defined with an embedding layer, LSTM layers, and a softmax output for multi-class classification.\n",
        "\n",
        "\n",
        "\n",
        "4. **Class Weights**:\n",
        "\n",
        "   - Class weights are calculated to handle imbalanced data.\n",
        "\n",
        "\n",
        "\n",
        "5. **Training**:\n",
        "\n",
        "   - The model is trained for 10 epochs with class weights to handle imbalanced classes.\n",
        "\n",
        "\n",
        "\n",
        "6. **Evaluation**:\n",
        "\n",
        "   - Model performance (loss, accuracy) is evaluated on the validation set.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This workflow cleans the data, builds a model, and trains it while handling class imbalance."
      ],
      "metadata": {
        "id": "xH1QKy7ag-Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the train.csv\n",
        "\n",
        "train_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/train.csv', encoding='cp1252')\n",
        "\n",
        "video_dir = '/kaggle/input/PES-ml-hack-link1/train_videos'\n",
        "\n",
        "\n",
        "\n",
        "# Function to get video file path from IDs\n",
        "\n",
        "def get_video_clip_path(row):\n",
        "\n",
        "    dialogue_id = row['Dialogue_ID']\n",
        "\n",
        "    utterance_id = row['Utterance_ID']\n",
        "\n",
        "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
        "\n",
        "    return os.path.join(video_dir, filename)\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function to get file paths for each sampled clip\n",
        "\n",
        "train_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Check sample paths\n",
        "\n",
        "print(train_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Load and preprocess the train.csv for video\n",
        "\n",
        "def preprocess_video_data(csv_path='/kaggle/input/PES-ml-hack-link1/train.csv'):\n",
        "\n",
        "    train_data = pd.read_csv(csv_path, encoding='cp1252')\n",
        "\n",
        "    train_data = train_data[['Utterance_ID', 'Dialogue_ID', 'Emotion']]\n",
        "\n",
        "    emotion_map = {'anger': 0, 'joy': 1, 'neutral': 2, 'sadness': 3, 'surprise': 4}\n",
        "\n",
        "    train_data['Emotion_Label'] = train_data['Emotion'].map(emotion_map)\n",
        "\n",
        "    return train_data\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Create CNN Model for feature extraction\n",
        "\n",
        "def create_cnn_model(input_shape=(64, 64, 3)):\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Extract frames from video\n",
        "\n",
        "def extract_frames_from_video(video_path, frame_size=(64, 64)):\n",
        "\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frames = []\n",
        "\n",
        "\n",
        "\n",
        "    while video_capture.isOpened():\n",
        "\n",
        "        ret, frame = video_capture.read()\n",
        "\n",
        "        if not ret:\n",
        "\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, frame_size)\n",
        "\n",
        "        frame = frame / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "\n",
        "\n",
        "    video_capture.release()\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Get video features using the CNN model\n",
        "\n",
        "def get_video_features(dialogue_id, utterance_id, cnn_model, video_dir='videos'):\n",
        "\n",
        "    video_filename = f'dia{dialogue_id}_utt{utterance_id}.mp4'\n",
        "\n",
        "    video_path = os.path.join(video_dir, video_filename)\n",
        "\n",
        "\n",
        "\n",
        "    if os.path.exists(video_path):\n",
        "\n",
        "        frames = extract_frames_from_video(video_path)\n",
        "\n",
        "        features = cnn_model.predict(frames)\n",
        "\n",
        "        return np.mean(features, axis=0)  # Use the average of the frame features\n",
        "\n",
        "    else:\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Prepare data for training the emotion recognition model\n",
        "\n",
        "def prepare_video_data(train_data, cnn_model, video_dir='videos'):\n",
        "\n",
        "    X_video = []\n",
        "\n",
        "    y = []\n",
        "\n",
        "\n",
        "\n",
        "    for _, row in train_data.iterrows():\n",
        "\n",
        "        features = get_video_features(row['Dialogue_ID'], row['Utterance_ID'], cnn_model, video_dir)\n",
        "\n",
        "        if features is not None:\n",
        "\n",
        "            X_video.append(features)\n",
        "\n",
        "            y.append(row['Emotion_Label'])\n",
        "\n",
        "\n",
        "\n",
        "    X_video = np.array(X_video)\n",
        "\n",
        "    y = np.array(y)\n",
        "\n",
        "    y = to_categorical(y, num_classes=5)\n",
        "\n",
        "\n",
        "\n",
        "    return X_video, y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6FzhCdJBf3Oq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T15:59:53.908134Z",
          "iopub.execute_input": "2024-11-11T15:59:53.908551Z",
          "iopub.status.idle": "2024-11-11T15:59:53.952456Z",
          "shell.execute_reply.started": "2024-11-11T15:59:53.908514Z",
          "shell.execute_reply": "2024-11-11T15:59:53.951358Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Video Data Preprocessing and Model Setup**\n",
        "\n",
        "\n",
        "\n",
        "1. **Load and Preprocess Data**:\n",
        "\n",
        "   - Loads `train.csv` containing video metadata.\n",
        "\n",
        "   - Generates video file paths based on `Dialogue_ID` and `Utterance_ID`.\n",
        "\n",
        "\n",
        "\n",
        "2. **Emotion Label Mapping**:\n",
        "\n",
        "   - Maps emotion labels (e.g., 'anger', 'joy') to numeric values using a dictionary.\n",
        "\n",
        "\n",
        "\n",
        "3. **CNN Model for Feature Extraction**:\n",
        "\n",
        "   - Defines a CNN model with 3 convolutional layers and max-pooling for feature extraction from video frames.\n",
        "\n",
        "\n",
        "\n",
        "4. **Extract Video Frames**:\n",
        "\n",
        "   - Extracts frames from video files, resizes to `(64, 64)`, and normalizes pixel values.\n",
        "\n",
        "\n",
        "\n",
        "5. **Video Feature Extraction**:\n",
        "\n",
        "   - Uses the CNN model to extract features from the video frames, averaging the features over all frames.\n",
        "\n",
        "\n",
        "\n",
        "6. **Prepare Data for Training**:\n",
        "\n",
        "   - Collects extracted video features and emotion labels, then prepares the dataset for training by converting labels to one-hot encoding.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qLC4BoeJhOns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create CNN model for feature extraction\n",
        "\n",
        "cnn_model = create_cnn_model(input_shape=(64, 64, 3))\n",
        "\n",
        "\n",
        "\n",
        "# Prepare video features and labels\n",
        "\n",
        "video_data = preprocess_video_data(csv_path='/kaggle/input/PES-ml-hack-link1/train.csv')\n",
        "\n",
        "X_video, y_video = prepare_video_data(video_data, cnn_model, video_dir='/kaggle/input/PES-ml-hack-link1/train_videos')\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets for video\n",
        "\n",
        "X_train_video, X_val_video, y_train_video, y_val_video = train_test_split(X_video, y_video, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Define the video model\n",
        "\n",
        "def create_video_model():\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu', input_shape=(X_video.shape[1],)))\n",
        "\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Create the video model\n",
        "\n",
        "video_model = create_video_model()\n",
        "\n",
        "\n",
        "\n",
        "# Train the video model\n",
        "\n",
        "history_video = video_model.fit(X_train_video, y_train_video, validation_data=(X_val_video, y_val_video), epochs=10, batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the video model on the validation set\n",
        "\n",
        "val_loss_video, val_accuracy_video = video_model.evaluate(X_val_video, y_val_video)\n",
        "\n",
        "print(f'Video Model - Validation Loss: {val_loss_video}, Validation Accuracy: {val_accuracy_video}')\n",
        "\n",
        "\n",
        "\n",
        "# Combine predictions from both models\n",
        "\n",
        "def combine_predictions(text_model, video_model, X_text, X_video):\n",
        "\n",
        "    text_predictions = text_model.predict(X_text)\n",
        "\n",
        "    video_predictions = video_model.predict(X_video)\n",
        "\n",
        "\n",
        "\n",
        "    # Average the predictions\n",
        "\n",
        "    combined_predictions = (text_predictions + video_predictions) / 2\n",
        "\n",
        "    return combined_predictions\n",
        "\n",
        "\n",
        "\n",
        "# Prepare validation data for combining predictions\n",
        "\n",
        "X_val_text = X_val\n",
        "\n",
        "X_val_video = X_val_video\n",
        "\n",
        "\n",
        "\n",
        "# Get combined predictions\n",
        "\n",
        "combined_predictions = combine_predictions(text_model, video_model, X_val_text, X_val_video)\n",
        "\n",
        "\n",
        "\n",
        "# Convert combined predictions to class labels\n",
        "\n",
        "final_predictions = np.argmax(combined_predictions, axis=1)"
      ],
      "metadata": {
        "id": "oOoL9gI9gGwM",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T16:00:00.797961Z",
          "iopub.execute_input": "2024-11-11T16:00:00.798409Z",
          "iopub.status.idle": "2024-11-11T16:06:02.073017Z",
          "shell.execute_reply.started": "2024-11-11T16:00:00.798362Z",
          "shell.execute_reply": "2024-11-11T16:06:02.071914Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Video and Text Model Training & Prediction Combination**\n",
        "\n",
        "\n",
        "\n",
        "1. **CNN Model for Feature Extraction**:\n",
        "\n",
        "   - A CNN model is created to extract features from video frames (`input_shape=(64, 64, 3)`).\n",
        "\n",
        "\n",
        "\n",
        "2. **Prepare Data**:\n",
        "\n",
        "   - Video features and emotion labels are prepared using `preprocess_video_data` and `prepare_video_data`.\n",
        "\n",
        "\n",
        "\n",
        "3. **Model Creation**:\n",
        "\n",
        "   - A simple fully connected video model is created with two Dense layers for classification. It is compiled using the Adam optimizer and categorical cross-entropy loss.\n",
        "\n",
        "\n",
        "\n",
        "4. **Training**:\n",
        "\n",
        "   - The video model is trained for 10 epochs with the training data, and its performance is evaluated on the validation set.\n",
        "\n",
        "\n",
        "\n",
        "5. **Combining Text and Video Predictions**:\n",
        "\n",
        "   - Predictions from both the text and video models are averaged to create a combined prediction for each sample.\n",
        "\n",
        "   - The final class label is determined by taking the argmax of the combined predictions.\n",
        "\n",
        "\n",
        "\n",
        "This approach merges the strengths of both text and video models by averaging their predictions to improve overall performance."
      ],
      "metadata": {
        "id": "XTanUWpVhdOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUNNING THE COMBINED MODEL ON TEST DATA**"
      ],
      "metadata": {
        "id": "oexkLJywXQXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/test.csv',encoding='cp1252')\n",
        "\n",
        "video_dir = '/kaggle/input/PES-ml-hack-link1/test_videos'\n",
        "\n",
        "\n",
        "\n",
        "# Function to get video file path from IDs\n",
        "\n",
        "def get_video_clip_path(row):\n",
        "\n",
        "    dialogue_id = row['Dialogue_ID']\n",
        "\n",
        "    utterance_id = row['Utterance_ID']\n",
        "\n",
        "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
        "\n",
        "    return os.path.join(video_dir, filename)\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function to get file paths for each sampled clip\n",
        "\n",
        "df['video_clip_path'] = df.apply(get_video_clip_path, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())\n",
        "\n",
        "\n",
        "\n",
        "def predict_on_test_data(df, text_model, video_model):\n",
        "\n",
        "\n",
        "\n",
        "    # Preprocess text\n",
        "\n",
        "    X_text_test = df['Utterance'].apply(preprocess_text)  # Use the same preprocessing as training\n",
        "\n",
        "    X_text_test_tokenized = tokenizer.texts_to_sequences(X_text_test)\n",
        "\n",
        "    X_text_test_padded = pad_sequences(X_text_test_tokenized, maxlen=100)\n",
        "\n",
        "\n",
        "\n",
        "    # Extract video features\n",
        "\n",
        "    X_video_test = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "\n",
        "        features = get_video_features(row['Dialogue_ID'], row['Utterance_ID'], cnn_model, video_dir)\n",
        "\n",
        "        if features is not None:\n",
        "\n",
        "            X_video_test.append(features)\n",
        "\n",
        "\n",
        "\n",
        "    X_video_test = np.array(X_video_test)\n",
        "\n",
        "\n",
        "\n",
        "    # Make predictions\n",
        "\n",
        "    text_predictions = text_model.predict(X_text_test_padded)\n",
        "\n",
        "    video_predictions = video_model.predict(X_video_test)\n",
        "\n",
        "\n",
        "\n",
        "    # Combine predictions\n",
        "\n",
        "    combined_predictions = (text_predictions + video_predictions) / 2\n",
        "\n",
        "    final_predictions = np.argmax(combined_predictions, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    # Map numerical predictions back to emotion labels\n",
        "\n",
        "    emotion_labels = label_encoder.inverse_transform(final_predictions)\n",
        "\n",
        "\n",
        "\n",
        "    return emotion_labels\n",
        "\n",
        "\n",
        "\n",
        "# Get predictions for the test data\n",
        "\n",
        "all_preds = predict_on_test_data(df, text_model, video_model)\n",
        "\n",
        "\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "\n",
        "all_ids = df[\"Sr No.\"]\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "\n",
        "    'Sr No.': all_ids,\n",
        "\n",
        "    'Emotion': all_preds\n",
        "\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "\n",
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "IE4uHXaWkMTW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-11T16:08:38.716146Z",
          "iopub.execute_input": "2024-11-11T16:08:38.716669Z",
          "iopub.status.idle": "2024-11-11T16:09:18.468989Z",
          "shell.execute_reply.started": "2024-11-11T16:08:38.716622Z",
          "shell.execute_reply": "2024-11-11T16:09:18.467939Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test Data Prediction and Submission Pipeline**\n",
        "\n",
        "\n",
        "\n",
        "1. **Load Test Data**:\n",
        "\n",
        "   - The test dataset (`test.csv`) is loaded, and paths to the corresponding video files are generated based on the `Dialogue_ID` and `Utterance_ID` using the `get_video_clip_path` function.\n",
        "\n",
        "\n",
        "\n",
        "2. **Text Data Preprocessing**:\n",
        "\n",
        "   - The `Utterance` column is preprocessed using the same steps as during training (lowercasing, tokenization, padding) to prepare text input for the model.\n",
        "\n",
        "\n",
        "\n",
        "3. **Extract Video Features**:\n",
        "\n",
        "   - For each test sample, video features are extracted using the pre-trained CNN model by calling the `get_video_features` function. These features are collected into `X_video_test`.\n",
        "\n",
        "\n",
        "\n",
        "4. **Make Predictions**:\n",
        "\n",
        "   - Both text and video models make predictions on the processed test data:\n",
        "\n",
        "     - `text_model.predict()` is used for text data predictions.\n",
        "\n",
        "     - `video_model.predict()` is used for video feature predictions.\n",
        "\n",
        "   - The predictions from both models are combined by averaging them.\n",
        "\n",
        "\n",
        "\n",
        "5. **Convert Predictions to Labels**:\n",
        "\n",
        "   - The combined predictions are converted to emotion labels by taking the `argmax` of the averaged predictions.\n",
        "\n",
        "   - The numerical predictions are then mapped back to emotion labels using `label_encoder.inverse_transform()`.\n",
        "\n",
        "\n",
        "\n",
        "6. **Prepare and Save Submission**:\n",
        "\n",
        "   - A DataFrame is created with the `Sr No.` from the test data and the predicted emotions.\n",
        "\n",
        "   - The submission DataFrame is saved to a CSV file for submission.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This pipeline preprocesses the test data, makes predictions using both text and video models, combines the results, and generates a final CSV file for submission."
      ],
      "metadata": {
        "id": "FXqcLLeOiEWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**                   THANK YOU**"
      ],
      "metadata": {
        "id": "YNWnQX_9iJM1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "QPfZ0acdPZCR"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}